{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667bc2f4-7fcd-445b-a4e7-d1d84b40ad2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXB1SiLP4OL"
      },
      "source": [
        "# simple RNN\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tzSWNYwxP4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3886dc08-b301-436e-d5ce-adca55f757ba"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.154928438969903\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "49 + 81 = 1\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.8965787368071358\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "45 + 62 = 90\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9082089715365554\n",
            "Pred:[1 1 0 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "103 + 1 = 204\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0658573965577998\n",
            "Pred:[0 0 0 0 1 1 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "11 + 103 = 12\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9728246229693249\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "111 + 100 = 254\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0052453064565543\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "15 + 68 = 0\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0522953133740902\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "75 + 119 = 148\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0023628719017548\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "120 + 78 = 176\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.8887856211429793\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "113 + 76 = 255\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.7018434795591424\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "57 + 57 = 112\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.7006685119290729\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "24 + 33 = 1\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.9482369076313502\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "59 + 98 = 117\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.7083213899946939\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "8 + 49 = 1\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.8289994429489853\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "96 + 53 = 133\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.5782226985268819\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "80 + 26 = 42\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.7280267233901245\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "118 + 20 = 106\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.8064016260104263\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "94 + 70 = 156\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.583135815685559\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "28 + 112 = 132\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.2690566052413506\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 0 0 0 0 1 1]\n",
            "3 + 0 = 1\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.4792044810813994\n",
            "Pred:[1 1 0 1 0 1 0 1]\n",
            "True:[1 1 0 1 0 1 1 1]\n",
            "125 + 90 = 213\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.45583214718361487\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "2 + 52 = 54\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.17216326786192124\n",
            "Pred:[1 1 0 1 1 0 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "124 + 93 = 217\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.38268104124099295\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "67 + 77 = 144\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.10865871696167932\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "82 + 83 = 165\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.5409507074072564\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "121 + 36 = 173\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.3876510678601209\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "99 + 13 = 96\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.16524259045635958\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "67 + 108 = 175\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.18644851386474784\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "62 + 52 = 114\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.28995933604831986\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "82 + 61 = 143\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.09758131136485457\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "35 + 15 = 50\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.07576947494790684\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "80 + 107 = 187\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.04757262583668117\n",
            "Pred:[0 0 1 0 1 1 0 1]\n",
            "True:[0 0 1 0 1 1 0 1]\n",
            "30 + 15 = 45\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.07871574244778255\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "13 + 113 = 126\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.04310145272356329\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "76 + 22 = 98\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.03189260108467934\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "100 + 96 = 196\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.12111735245335313\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "122 + 13 = 135\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.04300718311128974\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "6 + 120 = 126\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.10329343705442649\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "69 + 127 = 196\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.032184200496369386\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "92 + 71 = 163\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.04571485217778223\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "10 + 29 = 39\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.018779588092033665\n",
            "Pred:[0 0 0 1 1 1 0 1]\n",
            "True:[0 0 0 1 1 1 0 1]\n",
            "28 + 1 = 29\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.01015296010394391\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "99 + 54 = 153\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.02079998573107014\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "114 + 2 = 116\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.023662076462886417\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "22 + 33 = 55\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.02417090036790905\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "109 + 87 = 196\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.0025984484800542244\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "97 + 93 = 190\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.005652944119436322\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "43 + 40 = 83\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.014083417778040907\n",
            "Pred:[1 1 1 0 1 0 1 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "114 + 120 = 234\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.007531616768746833\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "31 + 68 = 99\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.01185840998610237\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "114 + 45 = 159\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.011481145297366796\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "44 + 98 = 142\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.005934558616923349\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "71 + 56 = 127\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.010309082049088382\n",
            "Pred:[1 1 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "72 + 120 = 192\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.005819348929549268\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "70 + 91 = 161\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.0017038768686341663\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "47 + 119 = 166\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.015304991089333253\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "118 + 41 = 159\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.008571230522386376\n",
            "Pred:[0 0 1 0 1 1 1 0]\n",
            "True:[0 0 1 0 1 1 1 0]\n",
            "46 + 0 = 46\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.00994603869829101\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "26 + 38 = 64\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.0074481371937213554\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "120 + 6 = 126\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.001639711215593791\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "43 + 31 = 74\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.007491955615225075\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "4 + 124 = 128\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.005853488248704117\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "33 + 110 = 143\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.0021439730596085277\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "39 + 57 = 96\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.005480186946585722\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "8 + 34 = 42\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.006055401456404547\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "46 + 2 = 48\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.004889099749196668\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "64 + 54 = 118\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.004195709823461031\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "81 + 126 = 207\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.002611799187493536\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "24 + 47 = 71\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.0026422483108252716\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "102 + 83 = 185\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.0011036303310186194\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "1 + 79 = 80\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.0025786658545750804\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "116 + 55 = 171\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.004713700659079955\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "82 + 34 = 116\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0009775584267863525\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "99 + 89 = 188\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0028448727512680352\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "14 + 85 = 99\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0018097363937616753\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "83 + 58 = 141\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.0019431399498450006\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "35 + 112 = 147\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.0017447784293604875\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "96 + 55 = 151\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.004047867416091014\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "58 + 12 = 70\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0015126494229038344\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "52 + 113 = 165\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.001665550524865937\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "42 + 39 = 81\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0014085727384413114\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "37 + 112 = 149\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0004430906876636438\n",
            "Pred:[0 0 0 1 1 0 1 0]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "23 + 3 = 26\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0034448309856673315\n",
            "Pred:[0 0 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "30 + 22 = 52\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0030404392899428126\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "124 + 12 = 136\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0013586105115088393\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "61 + 100 = 161\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.000575036084833328\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "27 + 123 = 150\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0012726509775780343\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "6 + 123 = 129\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.000264869949850664\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "65 + 37 = 102\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.0010622379184546539\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "53 + 40 = 93\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.002389936022290869\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "122 + 98 = 220\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0003622067769338396\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "43 + 23 = 66\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0005352098280911674\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "99 + 53 = 152\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0003672359890444825\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "101 + 71 = 172\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0014167815023873756\n",
            "Pred:[0 0 1 0 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "42 + 5 = 47\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0009339447874918864\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "67 + 112 = 179\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0005962985873531101\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "45 + 59 = 104\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.001020107854979062\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "82 + 3 = 85\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0013385735809444208\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "126 + 25 = 151\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0010613062938464022\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "18 + 15 = 33\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.00029735252978279426\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "47 + 31 = 78\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zQEPrtP4OP"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}