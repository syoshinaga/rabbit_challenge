{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN_after.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0b7116-3574-47f8-d83b-56417f2b32e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DNN_code')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "# simple RNN after\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbb347a0-d63d-4166-a0dc-2f5a36647a2f"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.4072530036077022\n",
            "Pred:[0 0 1 0 0 1 0 0]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "36 + 119 = 36\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9766471252601856\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "63 + 107 = 61\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.0229501341149865\n",
            "Pred:[0 0 1 0 1 1 1 0]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "44 + 111 = 46\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.255498388785072\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "30 + 67 = 158\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.978116182154551\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "64 + 50 = 68\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.9796955261597464\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "50 + 115 = 199\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.9167855979014842\n",
            "Pred:[1 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "104 + 19 = 231\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0767996075765427\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "115 + 74 = 4\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.870898972201082\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "115 + 117 = 254\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.153307667382534\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "85 + 53 = 255\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0031558709225945\n",
            "Pred:[0 0 0 0 0 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "82 + 10 = 5\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.9646559692891207\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "73 + 22 = 191\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.1338339991560078\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "19 + 43 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.6489610654305169\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "100 + 27 = 255\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.8549741602292498\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "47 + 64 = 125\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.7097812283596525\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "9 + 33 = 10\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.7784535459815118\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "56 + 126 = 252\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.6192188312976887\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "127 + 87 = 214\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.0884336791770381\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "14 + 117 = 100\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.870619119009152\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "78 + 17 = 110\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.6348355840090291\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "109 + 88 = 197\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.7954775434661971\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "0 + 92 = 100\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.5680281511434107\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "10 + 67 = 78\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.8370574580708481\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "34 + 86 = 176\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.6526940262776498\n",
            "Pred:[0 0 0 0 1 1 1 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "5 + 57 = 14\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.5859070838951211\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "112 + 79 = 255\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.21496393627889773\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "88 + 30 = 118\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.10035039809690242\n",
            "Pred:[1 1 1 0 1 0 1 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "113 + 121 = 234\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.23892528152804493\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "84 + 29 = 113\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.06907343130984579\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "57 + 60 = 117\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.2169836892567094\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "92 + 60 = 152\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.16849301482846057\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "59 + 73 = 132\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.0314700123737745\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "113 + 113 = 226\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.06923765995074073\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "13 + 83 = 96\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.06287690173828135\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "92 + 84 = 176\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.06987475105597934\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "105 + 21 = 126\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.05962887811799845\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "70 + 33 = 103\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.01911764050617928\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "13 + 63 = 76\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.015167782700342114\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "9 + 89 = 98\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.0646047390840908\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "58 + 14 = 72\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.024406290679562268\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "37 + 76 = 113\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.026054303146412123\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "17 + 126 = 143\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.022298429017302543\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "105 + 70 = 175\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.021473982452307843\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "38 + 55 = 93\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.03032771660973034\n",
            "Pred:[1 1 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "90 + 102 = 192\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.014444841659735601\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "127 + 72 = 199\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.005347134871078047\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "21 + 73 = 94\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.015330329336971366\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "101 + 46 = 147\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.0170174946022633\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "104 + 77 = 181\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.010566576246091785\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "123 + 24 = 147\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.012028432798174062\n",
            "Pred:[0 0 1 0 1 0 0 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "26 + 15 = 41\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.017133525082158065\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "4 + 70 = 74\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.016906543660697403\n",
            "Pred:[0 0 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "8 + 44 = 52\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.01608733200593804\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "50 + 0 = 50\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.0035509352751869915\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "107 + 107 = 214\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.0034141149205451247\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "91 + 19 = 110\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.01339004341612341\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "116 + 80 = 196\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.013909262797586735\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "76 + 10 = 86\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.013549218097615272\n",
            "Pred:[1 1 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "98 + 94 = 192\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.002200076157637962\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "61 + 41 = 102\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.009258482014929244\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "28 + 87 = 115\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.007337791926430775\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "56 + 83 = 139\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.0013107771152373448\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "99 + 13 = 112\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.005029925097722623\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "9 + 100 = 109\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.0069864240803469676\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "46 + 107 = 153\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.005123642929979717\n",
            "Pred:[1 1 0 1 0 1 0 1]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "121 + 92 = 213\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.0013711976561315384\n",
            "Pred:[1 0 1 0 1 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "69 + 99 = 168\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.004546444177623799\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "35 + 62 = 97\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.004228428394962637\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "67 + 20 = 87\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.003701130222109072\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "71 + 26 = 97\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.0007457145429492966\n",
            "Pred:[0 0 0 1 0 1 0 0]\n",
            "True:[0 0 0 1 0 1 0 0]\n",
            "3 + 17 = 20\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.0010226557322150123\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "9 + 107 = 116\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0038779980114893677\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "67 + 20 = 87\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0047195396020308575\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "106 + 115 = 221\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0008517186582622766\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "21 + 51 = 72\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.0005944484361131431\n",
            "Pred:[0 0 1 0 0 1 0 0]\n",
            "True:[0 0 1 0 0 1 0 0]\n",
            "1 + 35 = 36\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.003609331759034467\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "29 + 78 = 107\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0032868527955255866\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "104 + 33 = 137\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0036491484644168267\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "90 + 19 = 109\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0033559937487323624\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "115 + 58 = 173\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0032819680365766446\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "59 + 106 = 165\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0030465534441613544\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "28 + 87 = 115\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.002736103057400512\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "112 + 67 = 179\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0026755058852746248\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "114 + 69 = 183\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.001180289496638596\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "75 + 107 = 182\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0024352815352165886\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "47 + 98 = 145\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.005318826135765877\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "124 + 54 = 178\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.004560297940360321\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "96 + 12 = 108\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.002216771697802278\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "77 + 64 = 141\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.004094755990736666\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "64 + 76 = 140\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0040299562080143726\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "24 + 94 = 118\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.003971468491930792\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "116 + 12 = 128\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.00036983108694043764\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "109 + 7 = 116\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.00048519099147440316\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "31 + 113 = 144\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0042131411260393934\n",
            "Pred:[1 1 1 0 1 0 0 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "110 + 122 = 232\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.001689983942705227\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "15 + 70 = 85\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.00047605946576971607\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "89 + 73 = 162\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0022700012900384293\n",
            "Pred:[1 1 0 1 0 1 1 1]\n",
            "True:[1 1 0 1 0 1 1 1]\n",
            "126 + 89 = 215\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0021926896869136087\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "60 + 21 = 81\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0017870843899216809\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "117 + 70 = 187\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBt7vxmO-bn"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}