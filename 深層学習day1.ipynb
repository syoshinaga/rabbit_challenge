{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深層学習day1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWD9QQukq2Qd"
      },
      "source": [
        "# アジェンダ\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "ディープラーニングは、結局何をやろうとしているか2行以内で述べよ。  \r\n",
        "また、次の中のどの値の最適化が最終目的か。全て選べ。  \r\n",
        "①入力値[ X] ②出力値[ Y]③重み[W]④バイアス[b]⑤総入力[u] ⑥中間層入力[ z]⑦学習率[ρ]\r\n",
        "```  \r\n",
        "\r\n",
        "人間が自然に行うタスク(音声の認識、画像の特定、予測など)をコンピュータに学習させ、自動化すること。  \r\n",
        "最適化は③重み、④バイアス、⑦学習率に対して行う。\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "次のネットワークを紙にかけ。  \r\n",
        "入力層︓2ノード1層中間層︓３ノード2層出力層︓1ノード1層\r\n",
        "```\r\n",
        "\r\n",
        "https://github.com/syoshinaga/rabbit_challenge/blob/main/resource/day1_1.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnzTyI6-Z4pX"
      },
      "source": [
        "# Section1: 入力層〜中間層\r\n",
        "\r\n",
        "入力層はデータの入力が行われる。データは事前に数値化が必要。  \r\n",
        "中間層は入力層と出力層の中間にあり、その構成を変えることで複雑な問題を表現できるポテンシャルがある。  \r\n",
        "しかし段数を増やすと誤差逆伝播時に勾配消失の問題が出てくる。\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "この図式に動物分類の実例を入れてみよう。\r\n",
        "```\r\n",
        "https://github.com/syoshinaga/rabbit_challenge/tree/main/resource/day1_2.jpg\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "この数式をPythonで書け。\r\n",
        "```\r\n",
        "```\r\n",
        "u = np.dot(x, W) + b\r\n",
        "```\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "1-1のファイルから中間層の出力を定義しているソースを抜き出せ。\r\n",
        "```\r\n",
        "```  \r\n",
        "z = functions.relu(u)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2tMKbp7Z-qB"
      },
      "source": [
        "# Section2: 活性化関数\r\n",
        "\r\n",
        "単純パーセプトロン問題では出力層にのみ活性化関数を使用していた。  \r\n",
        "多層ニューラルネットワークでは中間層にも活性化関数を使うのが有効である。そうすることで非線形問題を表現可能になる。  \r\n",
        "中間層でよく使われる活性化関数はReLU関数、シグモイド関数、ステップ関数\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "線形と非線形の違いを図にかいて簡易に説明せよ。\r\n",
        "```\r\n",
        "https://github.com/syoshinaga/rabbit_challenge/blob/main/resource/day1_3.jpg\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "配布されたソースコードより該当する箇所を抜き出せ。\r\n",
        "```\r\n",
        "```\r\n",
        "z = functions.sigmoid(u)\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgspfpY8aCyb"
      },
      "source": [
        "# Section3: 出力層\r\n",
        "\r\n",
        "## 誤差関数\r\n",
        "\r\n",
        "二乗和誤差が有名、クラス分類ではクロスエントロピー誤差を使う。\r\n",
        "\r\n",
        "## 出力層の活性化関数\r\n",
        "\r\n",
        "分類問題の場合出力の総和を1にする必要があり、ソフトマックス関数を使う\r\n",
        "\r\n",
        "### 確認テスト\r\n",
        "```\r\n",
        "・なぜ、引き算でなく二乗するか述べよ  \r\n",
        "・下式の1/2はどういう意味を持つか述べよ\r\n",
        "```\r\n",
        " 引き算の場合、正の差分と負の差分とで打ち消し合ってしまうため  \r\n",
        " 微分したとき出てくる2を消す\r\n",
        "\r\n",
        "### 確認テスト\r\n",
        "```\r\n",
        "①~③の数式に該当するソースコードを示し、一行づつ処理の説明をせよ。\r\n",
        "```\r\n",
        "```\r\n",
        "def softmax(x):\r\n",
        "    if x.ndim == 2: // xが2次元の場合\r\n",
        "        x = x.T // xを転置\r\n",
        "        x = x - np.max(x, axis=0) // 各成分から最大値を引き算\r\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0) // 計算\r\n",
        "        return y.T //　転置したものを返す\r\n",
        "\r\n",
        "    x = x - np.max(x) # オーバーフロー対策\r\n",
        "    return np.exp(x) / np.sum(np.exp(x)) // 計算\r\n",
        "\r\n",
        "```\r\n",
        "### 確認テスト\r\n",
        "```\r\n",
        "①~②の数式に該当するソースコードを示し、一行づつ処理の説明をせよ。\r\n",
        "```\r\n",
        "```\r\n",
        "def cross_entropy_error(d, y):\r\n",
        "    if y.ndim == 1: // yが1次元の場合\r\n",
        "        d = d.reshape(1, d.size) // 1×dの形に整形\r\n",
        "        y = y.reshape(1, y.size) // 1×yの形に整形\r\n",
        "        \r\n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\r\n",
        "    if d.size == y.size: // dとyのサイズが等しい場合\r\n",
        "        d = d.argmax(axis=1) // dにaxis=1の方向の最大値を設定\r\n",
        "             \r\n",
        "    batch_size = y.shape[0] //バッチサイズとしてy.shape[0]を設定\r\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size // 計算\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KQN53cYaHSP"
      },
      "source": [
        "# Section4: 勾配降下法\r\n",
        "\r\n",
        "誤差関数を最小化するのに使用するアルゴリズム。  \r\n",
        "重みと誤差関数を重みで微分して得られる$\\nabla E$を用いて重みを更新していく。  \r\n",
        "学習率を大きくしすぎると収束しないので調整が必要。\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "該当するソースコードを探してみよう。\r\n",
        "```\r\n",
        "```\r\n",
        "network[key]  -= learning_rate * grad[key]\r\n",
        "```\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "オンライン学習とは何か2行でまとめよ\r\n",
        "```\r\n",
        "バッチ単位ではなくデータ1件ごとに重みを更新していく学習  \r\n",
        "変化に迅速に対応できるのと学習済データを蓄えておかなくていいメリットがある\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "・この数式の意味を図に書いて説明せよ。\r\n",
        "```\r\n",
        "誤差関数が最小となるように重みをミニバッチ単位で更新していく\r\n",
        "https://github.com/syoshinaga/rabbit_challenge/tree/main/resource/day1_4.jpg\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDxeeyYnaNfa"
      },
      "source": [
        "# Section5: 誤差逆伝搬法\r\n",
        "\r\n",
        "複雑の誤差関数の偏微分をコンピュータで有効的に計算しやすくするアルゴリズム。  \r\n",
        "出力層に近い層から順に重みの微分を求めていくので、そのフローから逆伝搬と呼ばれる。  \r\n",
        "層が増えると途中で勾配消失してしまう問題がある。\r\n",
        "\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "誤差逆伝播法では不要な再帰的処理を避ける事が出来る。  \r\n",
        "既に行った計算結果を保持しているソースコードを抽出せよ。\r\n",
        "```\r\n",
        "```\r\n",
        "delta2 = functions.d_sigmoid_with_loss(d, y)\r\n",
        "delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)\r\n",
        "```\r\n",
        "## 確認テスト\r\n",
        "```\r\n",
        "2つの空欄に該当するソースコードを探せ\r\n",
        "```\r\n",
        "```\r\n",
        "delta2 = functions.d_mean_squared_error(d, y)  \r\n",
        "grad['W2'] = np.dot(z1.T, delta2)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6wABYlFZr4I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}